{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Lk17QluX5SN"
   },
   "source": [
    "# **Text Analysis Techniques-Part2**\n",
    "\n",
    "## **Pragmatic Analysis**\n",
    "\n",
    "Pragmatic Analysis focuses on the intended meaning of text or speech, considering context, tone, speaker intent, and implied meanings. Unlike semantic analysis (which focuses on literal meaning), pragmatic analysis examines the broader context, such as the speaker's intent, cultural norms, or implied meanings.\n",
    "\n",
    "### **Key Concepts of Pragmatic Analysis:**\n",
    "\n",
    "  * Deixis: Resolving references like this, that, here, there based on the\n",
    "    context.\n",
    "  * Anaphora Resolution: Identifying antecedents for pronouns (e.g.,\n",
    "    resolving he to John in \"John is here. He is happy.\").\n",
    "  * Speech Acts: Understanding the function of a statement, such as\n",
    "    whether it’s a request, command, or question.\n",
    "  * Conversational Implicature: Inferring unstated meanings based on\n",
    "    context.\n",
    "\n",
    "\n",
    "#### **Applications of Pragmatic Analysis:**\n",
    "\n",
    "  * Sentiment and Emotion Detection: Identify tone or emotional intent (e.\n",
    "    g., sarcasm or anger).\n",
    "  * Politeness Analysis: Determine the politeness level in requests or\n",
    "    responses.\n",
    "  * Dialogue Understanding: Analyze implied meanings in conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZL10Nl0Z2lG"
   },
   "source": [
    "### **A.Deixis Resolution**\n",
    "\n",
    "Deixis resolution is a subfield of pragmatics in Natural Language Processing (NLP) that resolves references in language that depend on context to understand their meaning. Deictic expressions are words or phrases whose interpretation depends on the context of the utterance, such as:\n",
    "\n",
    "  * Person deixis: References to people (e.g., I, you, he).\n",
    "  * Place deixis: References to locations (e.g., here, there).\n",
    "  * Time deixis: References to time (e.g., now, then).\n",
    "  * Discourse deixis: References to parts of the discourse itself (e.g., this, that in \"This is important\").\n",
    "  * Social deixis: Words that convey social relationships (e.g., sir, madam).\n",
    "\n",
    "Deixis resolution involves determining the entity, location, or time to which a deictic expression refers, given the surrounding context.\n",
    "\n",
    "##### **Steps in Deixis Resolution**\n",
    "\n",
    "  1. Identify Deictic Expressions: Detect words or phrases that are\n",
    "     deictic in nature.\n",
    "  2. Extract Context: Gather contextual information such as speaker,\n",
    "     listener, time, and place.\n",
    "  3. Resolve the Referent: Map the deictic expression to its correct\n",
    "     referent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This Notebook runs on python 3.7 & spacy 2.1.0\n",
    "### Follow the steps\n",
    "### 1. Create a virual env in Conda:\n",
    "#### ! conda create -n pragmatics python=3.7\n",
    "####! conda activate pragmatics\n",
    "#### ! pip install spacy==2.1.0 transformers nltk neuralcoref\n",
    "###! python -m spacy download en_core_web_sm\n",
    "#### ! pip install ipykernel\n",
    "#### ! python -m ipykernel install --user --name=pragmatics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "bQQWm1EDZf6w",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Let’s take an example where we resolve person, place, and time deixis.\n",
    "import spacy\n",
    "from datetime import datetime, timedelta  # Import timedelta\n",
    "\n",
    "\n",
    "# Load SpaCy's NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Context information\n",
    "context = {\n",
    "    \"speaker\": \"Alice\",\n",
    "    \"listener\": \"Bob\",\n",
    "    \"location\": \"New York\",\n",
    "    \"time\": datetime(2024, 12, 3, 10, 0, 0),  # Current date and time\n",
    "}\n",
    "\n",
    "# Example text\n",
    "text = \"I told you that we should meet here tomorrow.\"\n",
    "doc = nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UWXd2Z0lbK2Q",
    "outputId": "bae6ed95-b99d-407f-c0ba-e7ff9193db98",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person Deixis Found: I\n",
      "Person Deixis Found: you\n",
      "Person Deixis Found: we\n",
      "Place Deixis Found: here\n",
      "Time Deixis Found: tomorrow\n"
     ]
    }
   ],
   "source": [
    "## We identify deictic expressions like I, you, here, tomorrow.\n",
    "# Deictic words to track\n",
    "person_deixis = [\"I\", \"you\", \"we\", \"he\", \"she\", \"they\"]\n",
    "place_deixis = [\"here\", \"there\", \"this place\", \"that place\"]\n",
    "time_deixis = [\"now\", \"then\", \"today\", \"tomorrow\", \"yesterday\"]\n",
    "\n",
    "# Extract deictic expressions\n",
    "for token in doc:\n",
    "    if token.text in person_deixis:\n",
    "        print(f\"Person Deixis Found: {token.text}\")\n",
    "    elif token.text in place_deixis:\n",
    "        print(f\"Place Deixis Found: {token.text}\")\n",
    "    elif token.text in time_deixis:\n",
    "        print(f\"Time Deixis Found: {token.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zblCkNlHbYM1",
    "outputId": "fefd5d4b-03db-45f7-94a6-56e6433ae6e7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deictic Word: I, Resolved Referent: Alice\n",
      "Deictic Word: you, Resolved Referent: Bob\n",
      "Deictic Word: we, Resolved Referent: Alice and Bob\n",
      "Deictic Word: here, Resolved Referent: New York\n",
      "Deictic Word: tomorrow, Resolved Referent: 2024-12-04 10:00:00\n"
     ]
    }
   ],
   "source": [
    "## Using the provided context, resolve the referents:\n",
    "# Function to resolve deixis\n",
    "def resolve_deixis(token, context):\n",
    "    if token.text in person_deixis:\n",
    "        if token.text == \"I\":\n",
    "            return context[\"speaker\"]\n",
    "        elif token.text == \"you\":\n",
    "            return context[\"listener\"]\n",
    "        elif token.text == \"we\":\n",
    "            return f\"{context['speaker']} and {context['listener']}\"\n",
    "    elif token.text in place_deixis:\n",
    "        if token.text == \"here\":\n",
    "            return context[\"location\"]\n",
    "    elif token.text in time_deixis:\n",
    "        if token.text == \"tomorrow\":\n",
    "            return context[\"time\"] + timedelta(days=1)\n",
    "        elif token.text == \"now\":\n",
    "            return context[\"time\"]\n",
    "    return None\n",
    "\n",
    "# Resolve deictic expressions\n",
    "for token in doc:\n",
    "    referent = resolve_deixis(token, context)\n",
    "    if referent:\n",
    "        print(f\"Deictic Word: {token.text}, Resolved Referent: {referent}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPTrXX5BcBmV"
   },
   "source": [
    "##### **Use Cases of Deixis Resolution**\n",
    "* Chatbots: Understanding context-dependent pronouns and references.\n",
    "* Dialogue Systems: Resolving temporal or spatial references for\n",
    "  scheduling and location-based tasks.\n",
    "* Machine Translation: Accurate translation of deictic expressions.\n",
    "* Assistive Technologies: Helping users understand ambiguous references.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iX5rkM0Xcm8g"
   },
   "source": [
    "### **B. Anaphora Resolution**\n",
    "\n",
    "Anaphora resolution is the process of identifying what a pronoun (e.g., he, she, it, they), noun phrase, or another referring expression refers to in a given text. It is a subfield of coreference resolution, which deals with linking all expressions referring to the same entity in a text.\n",
    "\n",
    "For example:\n",
    "\n",
    "In the sentence, \"John went to the store. He bought some milk.\", the pronoun he refers to John. Resolving this reference is an example of anaphora resolution.\n",
    "\n",
    "### **Applications of Anaphora Resolution**\n",
    "\n",
    "  * Chatbots: Maintaining conversational coherence by tracking\n",
    "    references.\n",
    "  * Text Summarization: Avoiding ambiguity in summarizations.\n",
    "  * Machine Translation: Translating pronouns correctly based on gender\n",
    "    or context.\n",
    "  * Question Answering Systems: Resolving references to provide accurate\n",
    "    answers.\n",
    "    \n",
    "### **Steps in Anaphora Resolution**\n",
    "\n",
    "  1. Identify Referring Expressions: Detect pronouns or noun phrases.\n",
    "  2. Extract Candidates: Identify potential antecedents (e.g., preceding\n",
    "     noun phrases).\n",
    "  3. Resolve References: Link referring expressions to their correct\n",
    "     antecedents using context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kd9R_pFDcmQk",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coreference Clusters:\n",
      "John: [John, He]\n",
      "\n",
      "Resolved Text:\n",
      "John went to the store. John bought some milk.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import neuralcoref\n",
    "\n",
    "# Load SpaCy model and add NeuralCoref\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# Example text\n",
    "text = \"John went to the store. He bought some milk.\"\n",
    "\n",
    "# Process text with NLP pipeline\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print Coreference Clusters\n",
    "print(\"Coreference Clusters:\")\n",
    "for cluster in doc._.coref_clusters:\n",
    "    print(cluster)\n",
    "\n",
    "# Replace references with resolved mentions\n",
    "resolved_text = doc._.coref_resolved\n",
    "print(\"\\nResolved Text:\")\n",
    "print(resolved_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Advanced Approaches**\n",
    "### **Using Transformers**\n",
    "More robust models like BERT can be fine-tuned for coreference resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ed5128166d4b1391b5ec0c050a5c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/543 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7df31752bfe42c88ffd0886afda4c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff8c00604bb4f9095a8103cbe2f29b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nielsr/coref-bert-base were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'coef', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at nielsr/coref-bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load coreference resolution model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nielsr/coref-bert-base\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"nielsr/coref-bert-base\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: al, Entity: Coreference Cluster 1, Score: 0.5046244263648987\n",
      "Word: ##ice, Entity: Coreference Cluster 1, Score: 0.5368682146072388\n",
      "Word: went, Entity: Coreference Cluster 1, Score: 0.7053787112236023\n",
      "Word: to, Entity: Coreference Cluster 1, Score: 0.6924053430557251\n",
      "Word: the, Entity: Coreference Cluster 1, Score: 0.7790126204490662\n",
      "Word: park, Entity: Coreference Cluster 2, Score: 0.5955913662910461\n",
      "Word: ., Entity: Coreference Cluster 1, Score: 0.6946769952774048\n",
      "Word: she, Entity: Coreference Cluster 1, Score: 0.6544292569160461\n",
      "Word: enjoyed, Entity: Coreference Cluster 1, Score: 0.7125582695007324\n",
      "Word: the, Entity: Coreference Cluster 1, Score: 0.7699970602989197\n",
      "Word: sunshine, Entity: Coreference Cluster 2, Score: 0.5046542286872864\n",
      "Word: ., Entity: Coreference Cluster 1, Score: 0.802509069442749\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"Alice went to the park. She enjoyed the sunshine.\"\n",
    "\n",
    "# Perform token classification\n",
    "coref_pipeline = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "results = coref_pipeline(text)\n",
    "\n",
    "# # Map label to a human-readable category\n",
    "label_mapping = {\"LABEL_0\": \"Coreference Cluster 1\",\n",
    "                \"LABEL_1\":'Coreference Cluster 2'}\n",
    "resolved_entities = [\n",
    "    {**result, \"entity\": label_mapping[result[\"entity\"]]} for result in results\n",
    "]\n",
    "\n",
    "for token in resolved_entities:\n",
    "    print(f\"Word: {token['word']}, Entity: {token['entity']}, Score: {token['score']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Output Interpretation**\n",
    "#### **Tokenization:**\n",
    "\n",
    "    The text is tokenized into smaller subwords using a tokenizer like WordPiece (common in BERT-based models).\n",
    "    Example: The name \"Alice\" is split into al and ##ice (where ## denotes a subword).\n",
    "#### **Entity Labels:**\n",
    "\n",
    "    LABEL_0: Likely used for generic tokens or tokens not directly involved in coreference clusters.\n",
    "    LABEL_1: Indicates a distinct class, potentially marking tokens as belonging to a coreference cluster.\n",
    "    \n",
    "#### **Coreference Clusters:**\n",
    "\n",
    "    Tokens with the same entity label (LABEL_1) are interpreted as part of the same coreference cluster.\n",
    "    Here, LABEL_1 is assigned to:\n",
    "    park (index: 6)\n",
    "    sunshine (index: 11)\n",
    "    This suggests these tokens might be semantically linked, but they do not help resolve the coreference directly.\n",
    "#### **Pronoun Resolution:**\n",
    "\n",
    "    Alice (al and ##ice) and she (index: 8) are both labeled as LABEL_0. This might suggest that the model isn't distinguishing them into separate clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: al, Entity: Coreference Cluster 1, Score: 0.5139123201370239\n",
      "Word: ##ice, Entity: Coreference Cluster 1, Score: 0.5804552435874939\n",
      "Word: met, Entity: Coreference Cluster 1, Score: 0.6211926341056824\n",
      "Word: na, Entity: Coreference Cluster 2, Score: 0.5000610947608948\n",
      "Word: ##ncy, Entity: Coreference Cluster 1, Score: 0.5165741443634033\n",
      "Word: at, Entity: Coreference Cluster 1, Score: 0.6829224228858948\n",
      "Word: park, Entity: Coreference Cluster 2, Score: 0.6562352180480957\n",
      "Word: ., Entity: Coreference Cluster 1, Score: 0.7098606824874878\n",
      "Word: she, Entity: Coreference Cluster 1, Score: 0.6490195989608765\n",
      "Word: enjoyed, Entity: Coreference Cluster 1, Score: 0.7154765725135803\n",
      "Word: the, Entity: Coreference Cluster 1, Score: 0.7610661387443542\n",
      "Word: sunshine, Entity: Coreference Cluster 2, Score: 0.5103151202201843\n",
      "Word: ., Entity: Coreference Cluster 1, Score: 0.7583706974983215\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"Alice met nancy at park.She enjoyed the sunshine.\"\n",
    "\n",
    "# Perform token classification\n",
    "coref_pipeline = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "results = coref_pipeline(text)\n",
    "\n",
    "# Display coreference results\n",
    "# # Map label to a human-readable category\n",
    "label_mapping = {\"LABEL_0\": \"Coreference Cluster 1\",\n",
    "                \"LABEL_1\":'Coreference Cluster 2'}\n",
    "resolved_entities = [\n",
    "    {**result, \"entity\": label_mapping[result[\"entity\"]]} for result in results\n",
    "]\n",
    "\n",
    "for token in resolved_entities:\n",
    "    print(f\"Word: {token['word']}, Entity: {token['entity']}, Score: {token['score']}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Na assigned to Cluster 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Challenges in Anaphora Resolution**\n",
    "    * Ambiguity: Sentences like \"John met Bob. He was happy.\" require deeper context to resolve.\n",
    "    * Long-Distance References: Pronouns referring to entities mentioned far earlier in the text.\n",
    "    * Gender and Number Agreement: Resolving references based on singular/plural and gender.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **C.Speech Act Classification**\n",
    "\n",
    "Speech Act Classification (SAC) involves categorizing an utterance based on its intended function in communication, such as requesting, commanding, informing, or questioning. It originates from Speech Act Theory, introduced by philosophers John Austin and John Searle, which classifies communication acts into:\n",
    "\n",
    "    * Locutionary Act: The literal meaning of the words spoken.\n",
    "    * Illocutionary Act: The intended function of the speech (e.g., a command, a request).\n",
    "    * Perlocutionary Act: The effect the utterance has on the listener (e.g., persuasion, motivation).\n",
    "SAC focuses on identifying the illocutionary act of an utterance.\n",
    "\n",
    "### **Categories of Speech Acts**\n",
    "\n",
    "Speech acts are typically divided into:\n",
    "\n",
    "    1.Assertives: Statements that describe the world (e.g., \"It is raining.\").\n",
    "    2.Directives: Commands or requests (e.g., \"Can you open the door?\").\n",
    "    3.Commissives: Commitments to future actions (e.g., \"I will call you tomorrow.\").\n",
    "    4.Expressives: Expressions of emotions or attitudes (e.g., \"I am sorry.\").\n",
    "    5.Declarations: Statements that change the state of the world (e.g., \"I hereby declare you husband and wife.\").\n",
    "\n",
    "### **Practical Use Cases**\n",
    "    o Building conversational agents (e.g., chatbots, virtual assistants).\n",
    "    o Improving sentiment and intent analysis.\n",
    "    o Enhancing human-computer interaction systems.\n",
    "    o Analyzing legal or political discourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7e692578c34f9abd3029dd8aa12d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b58a4b1d2274b899d3cdcd757849e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d38f55d684417cbd387fe5c9f96f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c9bf5591b940fc96041cb9640e4f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2e74ca0cf8401597a07e604ac359b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Can you help me with this?\n",
      "Speech Act: Directive\n",
      "--------------------------------------------------\n",
      "Sentence: I will call you tomorrow.\n",
      "Speech Act: Assertive (Positive)\n",
      "--------------------------------------------------\n",
      "Sentence: Thank you for your help.\n",
      "Speech Act: Expressive\n",
      "--------------------------------------------------\n",
      "Sentence: I hereby declare this building open.\n",
      "Speech Act: Declaration\n",
      "--------------------------------------------------\n",
      "Sentence: It is a beautiful day.\n",
      "Speech Act: Assertive (Positive)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load Spacy model for linguistic features\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load Hugging Face's sentiment-analysis pipeline for example purposes\n",
    "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased\")\n",
    "\n",
    "# Speech Act Classification function\n",
    "def classify_speech_act(text):\n",
    "    # Rule-based classification based on specific words/phrases\n",
    "    if any(keyword in text.lower() for keyword in [\"please\", \"can you\", \"could you\"]):\n",
    "        return \"Directive\"\n",
    "    elif any(keyword in text.lower() for keyword in [\"I promise\", \"I will\", \"I guarantee\"]):\n",
    "        return \"Commissive\"\n",
    "    elif any(keyword in text.lower() for keyword in [\"I apologize\", \"sorry\", \"thank you\"]):\n",
    "        return \"Expressive\"\n",
    "    elif any(keyword in text.lower() for keyword in [\"I declare\", \"hereby\"]):\n",
    "        return \"Declaration\"\n",
    "    else:\n",
    "        # Use sentiment analysis model for assertive classification\n",
    "        sentiment = classifier(text)[0][\"label\"]\n",
    "        if sentiment == \"NEGATIVE\":\n",
    "            return \"Assertive (Negative)\"\n",
    "        else:\n",
    "            return \"Assertive (Positive)\"\n",
    "\n",
    "# Test examples\n",
    "examples = [\n",
    "    \"Can you help me with this?\",\n",
    "    \"I will call you tomorrow.\",\n",
    "    \"Thank you for your help.\",\n",
    "    \"I hereby declare this building open.\",\n",
    "    \"It is a beautiful day.\"\n",
    "]\n",
    "\n",
    "# Classify each example\n",
    "for sentence in examples:\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Speech Act: {classify_speech_act(sentence)}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **D.Conversational Implicature**\n",
    "Conversational implicature, a concept introduced by philosopher H.P. Grice, refers to the implied meanings in conversation that go beyond the literal meaning of the words. It arises when speakers rely on shared context and the listener's ability to infer additional meaning based on conversational maxims.\n",
    "\n",
    "### **Grice's Maxims**\n",
    "Conversational implicatures are guided by four maxims of the Cooperative Principle, which state that speakers and listeners aim to communicate effectively and efficiently:\n",
    "\n",
    "    Maxim of Quality: Be truthful. Do not say what you believe to be false or lack evidence for.\n",
    "    Maxim of Quantity: Be as informative as required, but not overly detailed.\n",
    "    Maxim of Relation: Be relevant.\n",
    "    Maxim of Manner: Be clear, avoiding ambiguity and unnecessary complexity.\n",
    "When a speaker flouts (intentionally violates) one of these maxims, it generates implicature.\n",
    "\n",
    "## **Examples of Implicature**\n",
    "    Flouting the Maxim of Quantity:\n",
    "\n",
    "        Speaker A: \"Did you finish your homework?\"\n",
    "        Speaker B: \"I started it.\"\n",
    "        Implicature: Speaker B hasn't finished their homework.\n",
    "    Flouting the Maxim of Relation:\n",
    "\n",
    "        Speaker A: \"How was the party?\"\n",
    "        Speaker B: \"The food was great!\"\n",
    "        Implicature: The party itself might not have been enjoyable, but the food was good.\n",
    "        \n",
    "## **Use Cases of Conversational Implicature**\n",
    "    o Natural Language Understanding for chatbots.\n",
    "    o Sarcasm and humor detection.\n",
    "    o Context-aware question answering systems.\n",
    "    o Sentiment analysis and emotion detection.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Did you like the movie?\n",
      "Response: The popcorn was delicious.\n",
      "Implicature: No obvious implicature detected.\n",
      "--------------------------------------------------\n",
      "Context: Are you coming to the meeting?\n",
      "Response: I have a lot of work to do.\n",
      "Implicature: No obvious implicature detected.\n",
      "--------------------------------------------------\n",
      "Context: Is the report ready?\n",
      "Response: I started working on it.\n",
      "Implicature: Possible implicature: Response may not be directly relevant to the context.\n",
      "--------------------------------------------------\n",
      "Context: How was your weekend?\n",
      "Response: Yes.\n",
      "Implicature: Literal response detected. No implicature.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umaMaheswari\\anaconda3\\envs\\pragmatics\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\umaMaheswari\\anaconda3\\envs\\pragmatics\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "C:\\Users\\umaMaheswari\\anaconda3\\envs\\pragmatics\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load Spacy language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def detect_implicature(context, response):\n",
    "    \"\"\"\n",
    "    Detect implicature in a conversation.\n",
    "    \n",
    "    Parameters:\n",
    "    context (str): The question or previous statement.\n",
    "    response (str): The speaker's response.\n",
    "    \n",
    "    Returns:\n",
    "    str: The inferred implicature.\n",
    "    \"\"\"\n",
    "    # Analyze the response with NLP\n",
    "    response_doc = nlp(response)\n",
    "    context_doc = nlp(context)\n",
    "    \n",
    "    # Check for flouting Quantity\n",
    "    if \"yes\" in response.lower() or \"no\" in response.lower():\n",
    "        return \"Literal response detected. No implicature.\"\n",
    "    elif any(token.text.lower() == \"but\" for token in response_doc):\n",
    "        return \"Possible implicature: Speaker may be contrasting or implying a limitation.\"\n",
    "    \n",
    "    # Check for flouting Relation\n",
    "    if response_doc.similarity(context_doc) < 0.3:  # Low similarity might indicate irrelevance\n",
    "        return \"Possible implicature: Response may not be directly relevant to the context.\"\n",
    "    \n",
    "    # Default case: No clear implicature\n",
    "    return \"No obvious implicature detected.\"\n",
    "\n",
    "# Test examples\n",
    "examples = [\n",
    "    (\"Did you like the movie?\", \"The popcorn was delicious.\"),\n",
    "    (\"Are you coming to the meeting?\", \"I have a lot of work to do.\"),\n",
    "    (\"Is the report ready?\", \"I started working on it.\"),\n",
    "    (\"How was your weekend?\", \"Yes.\")\n",
    "]\n",
    "\n",
    "# Analyze implicatures\n",
    "for context, response in examples:\n",
    "    print(f\"Context: {context}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"Implicature: {detect_implicature(context, response)}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBneSExsYkt1"
   },
   "source": [
    "### **Challenges of Pragmatic Analysis**\n",
    "    # Ambiguity: Pragmatic analysis often involves resolving multiple layers of ambiguity.\n",
    "    # Contextual Dependency: Requires external information such as world knowledge, user-specific data, or cultural norms.\n",
    "    # Complexity: Combining syntactic, semantic, and pragmatic features requires advanced models and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwn0_KY9Xy9d",
    "tags": []
   },
   "source": [
    "## **Discourse Integration**\n",
    "\n",
    "Discourse Integration refers to the process of understanding a sequence of sentences or utterances in a coherent manner. Unlike sentence-level analysis, it considers relationships between sentences to derive meaning and context. It aims to establish a logical structure in a conversation or document, connecting ideas and resolving ambiguities that require cross-sentence understanding.\n",
    "\n",
    "### **Key Concepts in Discourse Integration**\n",
    "\n",
    "    Coherence: Ensuring that the text flows logically. This includes resolving references, understanding implicit relationships, and maintaining topic continuity.\n",
    "    Coreference Resolution: Identifying when different expressions refer to the same entity (e.g., \"Alice\" and \"she\").\n",
    "    Temporal Sequencing: Understanding the order of events described in the text.\n",
    "    Discourse Relations: Relationships between clauses or sentences, such as contrast, cause-effect, or elaboration.\n",
    "    Explicit Relations: Indicated by discourse markers like \"however,\" \"therefore,\" \"because.\"\n",
    "    Implicit Relations: Inferred from context.\n",
    "    \n",
    "### **Applications of Discourse Integration**\n",
    "\n",
    "    Text Summarization: Extracting coherent and meaningful summaries.\n",
    "    Question Answering Systems: Resolving ambiguities by understanding the full discourse context.\n",
    "    Dialogue Systems: Maintaining topic continuity in conversations.\n",
    "    Machine Translation: Preserving coherence across sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 1: Coreference Resolution with Spacy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "Alice went to the park. She enjoyed the sunshine and decided to read a book. Her friend Nancy joined her later.\n",
      "\n",
      "\n",
      "Coreference Resolved Text:\n",
      "\n",
      "Alice went to the park. \n",
      "Alice enjoyed the sunshine and decided to read a book. \n",
      "Alice friend Nancy joined \n",
      "Alice later.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import neuralcoref\n",
    "\n",
    "# Load Spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add NeuralCoref for coreference resolution\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "# Input text\n",
    "text = \"\"\"\n",
    "Alice went to the park. She enjoyed the sunshine and decided to read a book. Her friend Nancy joined her later.\n",
    "\"\"\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print resolved text\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nCoreference Resolved Text:\")\n",
    "print(doc._.coref_resolved)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2: Temporal Sequencing and Discourse Relations**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c9e8225e00408cae33b3651578a90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/688 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73b90b5d37944d5fb42b8cc306681914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0afefa80668d4d799f0d598bc17744c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254248a7b9614bb9aaad568b7c1bccd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9353f932154df0aeb00c33f5bc7c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86ee7dd07f3405b96e7b3d82e8e0a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discourse Relation:\n",
      "Sentence 1: Alice went to the park.\n",
      "Sentence 2: She enjoyed the sunshine.\n",
      "Predicted Relation: NEUTRAL\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a pipeline for natural language inference (NLI)\n",
    "nli_pipeline = pipeline(\"text-classification\", model=\"FacebookAI/roberta-large-mnli\")\n",
    "\n",
    "# Input sentences\n",
    "sentence1 = \"Alice went to the park.\"\n",
    "sentence2 = \"She enjoyed the sunshine.\"\n",
    "\n",
    "# Combine sentences and predict their relation\n",
    "discourse_input = f\"{sentence1} {sentence2}\"\n",
    "result = nli_pipeline(discourse_input)\n",
    "\n",
    "# Display results\n",
    "print(\"Discourse Relation:\")\n",
    "print(f\"Sentence 1: {sentence1}\")\n",
    "print(f\"Sentence 2: {sentence2}\")\n",
    "print(f\"Predicted Relation: {result[0]['label']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NOTE**\n",
    "\n",
    "In Natural Language Inference, relations between pairs of sentences (premises and hypotheses) are typically classified as:\n",
    "\n",
    "    Entailment: The second sentence logically follows from the first.\n",
    "\n",
    "    Example:\n",
    "        Premise: \"Alice went to the park.\"\n",
    "        Hypothesis: \"Alice was outdoors.\"\n",
    "        Relation: Entailment.\n",
    "        Contradiction: The second sentence is logically inconsistent with the first.\n",
    "\n",
    "    Example:\n",
    "        Premise: \"Alice went to the park.\"\n",
    "        Hypothesis: \"Alice stayed home all day.\"\n",
    "        Relation: Contradiction.\n",
    "        Neutral: There is no clear entailment or contradiction; the sentences are unrelated or loosely connected.\n",
    "#### **Why is \"NEUTRAL\" Predicted?**\n",
    "\n",
    "The neutral prediction suggests that the model recognizes:\n",
    "\n",
    "    No Explicit Causal Link: The action of going to the park does not guarantee that Alice enjoyed the sunshine. She might have gone for other reasons, such as   exercise or meeting a friend.\n",
    "    Contextual Ambiguity: Without additional context, the enjoyment of sunshine might be an independent observation rather than a consequence of her going to the park.\n",
    "    Weak Semantic Overlap: The words \"park\" and \"sunshine\" do not strongly relate in a way that suggests a cause-effect or entailment relationship.\n",
    "    \n",
    "\n",
    "The \"NEUTRAL\" label reflects that there’s no inherent semantic or logical connection between the sentences strong enough for entailment or contradiction. This outcome often happens in casual, loosely connected discourse, especially when no explicit markers indicate causality or relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### **Step 3: Custom Discourse Parsing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discourse Relation: Cause\n",
      "Sentence: Alice went to the park because she wanted some fresh air\n",
      "--------------------------------------------------\n",
      "Discourse Relation: Sequence\n",
      "Sentence: Later, she met Nancy\n",
      "--------------------------------------------------\n",
      "Discourse Relation: Contrast\n",
      "Sentence: However, it started raining\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def analyze_discourse(text):\n",
    "    \"\"\"\n",
    "    Analyze discourse relations based on explicit markers.\n",
    "    \"\"\"\n",
    "    discourse_markers = {\n",
    "        \"cause\": [\"because\", \"since\", \"as\"],\n",
    "        \"contrast\": [\"however\", \"but\", \"although\"],\n",
    "        \"sequence\": [\"then\", \"after\", \"later\"],\n",
    "        \"addition\": [\"and\", \"also\", \"furthermore\"]\n",
    "    }\n",
    "    \n",
    "    sentences = text.split(\".\")\n",
    "    for sentence in sentences:\n",
    "        for relation, markers in discourse_markers.items():\n",
    "            if any(marker in sentence.lower() for marker in markers):\n",
    "                print(f\"Discourse Relation: {relation.capitalize()}\")\n",
    "                print(f\"Sentence: {sentence.strip()}\")\n",
    "                print(\"-\" * 50)\n",
    "\n",
    "# Input text\n",
    "text = \"\"\"\n",
    "Alice went to the park because she wanted some fresh air. Later, she met Nancy. However, it started raining.\n",
    "\"\"\"\n",
    "\n",
    "analyze_discourse(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Challenges in Discourse Integration**\n",
    "\n",
    "    1. Ambiguity of Discourse Relations\n",
    "    \n",
    "        Issue: Many discourse relations are not explicitly marked in a text, making it difficult for models to infer the intended meaning.\n",
    "        Example: In the sentence \"Alice went to the park. She enjoyed the sunshine,\" there is no explicit marker such as \"because\" or \"however\" to signal the relationship between the two sentences. It is unclear whether the enjoyment of sunshine is the reason for going to the park or simply an independent statement.\n",
    "        Solution: Disambiguating such cases requires deep contextual understanding, which can be difficult to achieve without advanced models or additional context.\n",
    "        \n",
    "    2. Coreference and Anaphora Resolution\n",
    "    \n",
    "        Issue: Properly linking pronouns or other referring expressions to their antecedents in a discourse is a major challenge. Incorrect coreference resolution can lead to incoherent text and errors in understanding.\n",
    "        Example: In \"Alice went to the park. She enjoyed the sunshine,\" resolving \"She\" to \"Alice\" is crucial to understanding the text correctly.\n",
    "        Solution: Accurate coreference resolution techniques are necessary for discourse integration, but they require sophisticated algorithms that can track entities across multiple sentences.\n",
    "        \n",
    "    3. Long-Range Dependencies\n",
    "    \n",
    "        Issue: Discourse relations are often spread over longer distances within a text. For example, the cause and effect relationship might not be contained within two adjacent sentences but might span several sentences or even paragraphs.\n",
    "        Example: \"Alice went to the park. She decided to read a book. After a while, it started raining. She was frustrated and went back home.\"\n",
    "        Solution: To handle long-range dependencies, models need to capture information from the broader context, which can be computationally intensive. Transformer-based models like BERT and GPT are good at handling these long-range dependencies but still have limitations.\n",
    "        \n",
    "    4. Implicit Discourse Relations\n",
    "    \n",
    "        Issue: Many discourse relations are implicit rather than explicitly marked by conjunctions or discourse markers. In these cases, understanding the relationship between sentences relies on interpreting the context.\n",
    "        Example: \"Alice went to the park. She was happy.\" The relationship between happiness and the park is not explicitly mentioned, but it may be inferred as cause-effect.\n",
    "        Solution: Models must be able to infer implicit relations, which is a challenging aspect of discourse integration. Leveraging pre-trained models like BERT, which understand context better, can help with implicit relation detection.\n",
    "\n",
    "    5. Complexity of Discourse Structures\n",
    "    \n",
    "        Issue: Discourse structures are often complex and can involve nested relations, multiple arguments, and non-linear sequences of events. Modeling such structures requires sophisticated algorithms.\n",
    "        Example: \"Alice went to the park, and after meeting Bob, they went for coffee. But Alice wasn't happy with the coffee because it was too cold.\"\n",
    "        Solution: Discourse parsing algorithms, which identify and represent discourse relations, are required to capture the complexity. Tree-based methods like Rhetorical Structure Theory (RST) parsing or neural discourse parsers are often used but can be computationally expensive.\n",
    "\n",
    "    6. Cultural and Contextual Sensitivity\n",
    "    \n",
    "        Issue: Discourse relations can vary across cultures, regions, and contexts. What is considered a logical relationship in one context may not hold in another.\n",
    "        Example: In some cultures, indirect speech acts (like using hints or polite requests) might be interpreted differently, impacting the discourse interpretation.\n",
    "        Solution: Training models on diverse and contextually rich datasets, or fine-tuning models for specific cultural or regional nuances, can help alleviate this issue.\n",
    "\n",
    "    7. Scalability and Efficiency\n",
    "    \n",
    "        Issue: Discourse integration often requires processing entire documents or large text corpora, making it computationally expensive. Maintaining coherence across large bodies of text is resource-intensive.\n",
    "        Example: Resolving discourse relations in a long document, such as a book or research paper, requires handling long-range dependencies, coreference, and discourse relations across many sentences or paragraphs.\n",
    "        Solution: Optimizing the models for efficiency, such as through pruning, distillation, or the use of more efficient transformer models (like Longformer or Reformer), can help with scalability.\n",
    "\n",
    "    8. Noise and Inconsistent Discourse Markers\n",
    "    \n",
    "        Issue: Real-world data often contains noisy or inconsistent discourse markers. Text from social media, conversations, or informal writing might lack the grammatical markers necessary for proper discourse integration.\n",
    "        Example: A sentence like \"I went to the store. Got some milk.\" lacks a proper conjunction like \"and\" or \"then\" but may still be logically coherent.\n",
    "        Solution: Handling noisy data involves robust preprocessing and leveraging unsupervised or semi-supervised learning techniques, which can identify patterns in unstructured or informal text.\n",
    "\n",
    "    9. Multimodal Discourse Integration\n",
    "    \n",
    "        Issue: Many real-world applications (such as dialogue systems, video captioning, or social media analysis) require integrating not just text but also other modalities (e.g., images, audio, gestures).\n",
    "        Example: In a video conversation, a speaker might nod or gesture while speaking, adding context to the discourse that isn't explicitly present in the text.\n",
    "        Solution: Multimodal NLP techniques that combine textual, visual, and auditory cues can be applied. For instance, combining image captioning with dialogue analysis to resolve references and maintain discourse coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pragmatics",
   "language": "python",
   "name": "pragmatics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
